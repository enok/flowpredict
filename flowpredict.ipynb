{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enok/flowpredict/blob/main/flowpredict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "614a15ed",
      "metadata": {
        "id": "614a15ed"
      },
      "source": [
        "# Free Cash Flow Forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30479985",
      "metadata": {
        "id": "30479985"
      },
      "source": [
        "<small><div style=\"text-align: right\">The data will consist of quarterly reports spanning from 2012 to 2022.</div></small>\n",
        "<small><div style=\"text-align: right\">The forecasting will be from 2023 to 2025.</div></small>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5542614c",
      "metadata": {
        "id": "5542614c"
      },
      "source": [
        "## 1. Downloading and loading stock market data from the CVM website."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d719529",
      "metadata": {
        "id": "6d719529"
      },
      "source": [
        "### Companies to be analised\n",
        "\n",
        "\n",
        "| Company       | Ticker    | CVM Code  |\n",
        "| :---          | :----     | ----:     |\n",
        "| CPFL Energia  | CPFE3     | 18660     |\n",
        "| Cemig         | CMIG4     | 2453      |\n",
        "| Eletrobras    | ELET6     | 2437      |\n",
        "| Enev          | ENEV3     | 21237     |\n",
        "| Taesa         | TAEE11    | 20257     |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7b914934",
      "metadata": {
        "id": "7b914934",
        "outputId": "2a242971-17aa-45bf-cc5c-5720d1abb2aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "companies: {18660: {'name': 'CPFL Energia', 'ticker': 'CPFE3'}, 2453: {'name': 'Cemig', 'ticker': 'CMIG4'}, 2437: {'name': 'Eletrobras', 'ticker': 'ELET6'}, 21237: {'name': 'Enev', 'ticker': 'ENEV3'}, 20257: {'name': 'Taesa', 'ticker': 'TAEE11'}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "companies = {\n",
        "    18660: {\"name\": \"CPFL Energia\", \"ticker\": \"CPFE3\"},\n",
        "    2453:  {\"name\": \"Cemig\", \"ticker\": \"CMIG4\"},\n",
        "    2437:  {\"name\": \"Eletrobras\", \"ticker\": \"ELET6\"},\n",
        "    21237: {\"name\": \"Enev\", \"ticker\": \"ENEV3\"},\n",
        "    20257: {\"name\": \"Taesa\", \"ticker\": \"TAEE11\"}\n",
        "}\n",
        "\n",
        "print(f'companies: {companies}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca138e55",
      "metadata": {},
      "source": [
        "### Accounts selected\n",
        "\n",
        "Based on:<br>\n",
        "Investiment Valuation Tools and Techniques for Determining the Value of Any Asset (Aswath Damodaran)<br>\n",
        "    CHAPTER 14 - Free Cash Flow to Equity Discount Models<br><br>\n",
        "\n",
        "*Free Cash Flows to Equity (FCFE) = <br>\n",
        "\tNet income - (Capital expenditures - Depreciation) <br>\n",
        "\t- (Change in noncash working capital) <br>\n",
        "\t+ (New debt issued - Debt repayments)*<br><br>\n",
        "\n",
        "\n",
        "*Fluxo de Caixa Livre para os Acionistas (FCLPA) = <br>\n",
        "Lucro Líquido<br>\n",
        " \\- (Investimentos em Ativos Fixos - Depreciação)<br>\n",
        " \\- (Variação no Capital de Giro, excluindo Caixa)<br>\n",
        " \\+ (Novas Dívidas Emitidas - Pagamentos de Dívida)*<br><br>\n",
        "\n",
        "*FCFE = <br>\n",
        "  3.11 (Lucro/Prejuizo Consolidado do Periodo do DRE)<br>\n",
        "  \\- (Δ 1.02.03 (Imobilizado) - 7.04.01 (Depreciacao, Amortizacao e Exaustao da DVA))<br>\n",
        "  \\- (Δ 1.01 (Ativo Circulante) - Δ 1.01.01 (Caixa e Equivalentes de Caixa) -  Δ 2.01 (Passivo Circulante))<br>\n",
        "  \\+ (Δ 2.01.04(Emprestimos e Financiamentos Circulantes) + Δ 2.02.01(Emprestimos e Financiamentos Nao Circulantes))*<br><br>\n",
        "\n",
        "\n",
        "\n",
        "**BPA**\n",
        "\n",
        "| Code       | Description                                           |\n",
        "| :---       | :----                                                 |\n",
        "| 1.01       | Ativo Circulante                                      |\n",
        "| 1.01.01    | Caixa e Equivalentes de Caixa                         |\n",
        "| 1.02.03    | Imobilizado                                           |\n",
        "\n",
        "<br>                                                                \n",
        "\n",
        "**BPP**\n",
        "\n",
        "| Code       | Description                                           |\n",
        "| :---       | :----                                                 |\n",
        "| 2.01       | Passivo Circulante                                    |\n",
        "| 2.01.04    | Empréstimos e Financiamentos (Passivo Circulante)     |\n",
        "| 2.02.01    | Empréstimos e Financiamentos (Passivo Nao Circulante) |\n",
        "\n",
        "<br>                                                                \n",
        "                                                                    \n",
        "**DRE**\n",
        "\n",
        "| Code       | Description                                           |\n",
        "| :---       | :----                                                 |\n",
        "| 3.11       | Lucro/Prejuízo Consolidado do Período                 |\n",
        "\n",
        "<br>                                                                \n",
        "\n",
        "**DVA**\n",
        "                                                     \n",
        "| Code       | Description                                           |\n",
        "| :---       | :----                                                 |\n",
        "| 7.04.01    | Depreciação, Amortização e Exaustão                   |\n",
        "\n",
        "\n",
        "<small><div style=\"text-align: right\">Source: <a>https://www.rad.cvm.gov.br/ENET/documentos/EspecificacaoArquivoXML_InformacoesTrimestrais.xlsx<a></div></small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b113fe5d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file_types selected: ['BPA', 'BPP', 'DRE', 'DVA']\n",
            "accounts selected: ['1.01', '1.01.01', '1.02.03', '2.01', '2.01.04', '2.02.01', '3.11', '7.04.01']\n"
          ]
        }
      ],
      "source": [
        "file_types = ['BPA', 'BPP', 'DRE', 'DVA']\n",
        "print(f'file_types selected: {file_types}')\n",
        "\n",
        "account_to_be_kept = ['1.01','1.01.01','1.02.03',\n",
        "                    '2.01','2.01.04','2.02.01',\n",
        "                    '3.11',\n",
        "                    '7.04.01']\n",
        "print(f'accounts selected: {account_to_be_kept}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "865c15f2",
      "metadata": {},
      "source": [
        "### Interval of years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "61c7c69d",
      "metadata": {},
      "outputs": [],
      "source": [
        "years = range(2012, 2023)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e587cc01",
      "metadata": {
        "id": "e587cc01"
      },
      "source": [
        "### Installing libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "24129d8a",
      "metadata": {
        "id": "24129d8a",
        "outputId": "7c9b678c-e9d4-4fc6-a867-c678a6c13074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: requests in /usr/lib/python3.11/site-packages (2.28.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3.11/site-packages (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/lib/python3.11/site-packages (from requests) (2.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.11/site-packages (from requests) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.11/site-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.11/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: chardet in /usr/lib/python3.11/site-packages (5.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: files in /home/enok/.local/lib/python3.11/site-packages (1.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: chardet in /usr/lib/python3.11/site-packages (5.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install requests beautifulsoup4\n",
        "%pip install chardet\n",
        "%pip install files\n",
        "%pip install chardet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "805f8d4e",
      "metadata": {
        "id": "805f8d4e"
      },
      "source": [
        "### Importing libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b03bf973",
      "metadata": {
        "id": "b03bf973"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import chardet\n",
        "from urllib.parse import urljoin\n",
        "from bs4 import BeautifulSoup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25f88ec",
      "metadata": {
        "id": "b25f88ec"
      },
      "source": [
        "### Setting Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "aaf70850",
      "metadata": {
        "id": "aaf70850"
      },
      "outputs": [],
      "source": [
        "#pd.set_option('display.max_columns', 10)\n",
        "pd.reset_option('display.max_columns')\n",
        "pd.set_option('display.width', 200)\n",
        "#pd.reset_option('display.width')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb55058e",
      "metadata": {
        "id": "bb55058e"
      },
      "source": [
        "### Downloading balance sheets files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6563bd89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6563bd89",
        "outputId": "b2c3d71e-27dc-4d52-ace6-755b7f4f6077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: itr_cia_aberta_2012.zip\n",
            "Downloading: itr_cia_aberta_2013.zip\n",
            "Downloading: itr_cia_aberta_2014.zip\n",
            "Downloading: itr_cia_aberta_2015.zip\n",
            "Downloading: itr_cia_aberta_2016.zip\n",
            "Downloading: itr_cia_aberta_2017.zip\n",
            "Downloading: itr_cia_aberta_2018.zip\n",
            "Downloading: itr_cia_aberta_2019.zip\n",
            "Downloading: itr_cia_aberta_2020.zip\n",
            "Downloading: itr_cia_aberta_2021.zip\n",
            "Downloading: itr_cia_aberta_2022.zip\n",
            "Unzipping: itr_cia_aberta_2012.zip -> downloaded_files/itr_cia_aberta_2012\n",
            "CSV type: 2012\n",
            "Removing: downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_2012.csv, its not in the allowed list: ['BPA', 'BPP', 'DRE', 'DVA']\n",
            "CSV type: BPA\n",
            "CSV type: BPA\n",
            "CSV type: BPP\n",
            "CSV type: BPP\n",
            "CSV type: DFC\n",
            "Removing: downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_DFC_MD_con_2012.csv, its not in the allowed list: ['BPA', 'BPP', 'DRE', 'DVA']\n",
            "CSV type: DFC\n",
            "Removing: downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_DFC_MD_ind_2012.csv, its not in the allowed list: ['BPA', 'BPP', 'DRE', 'DVA']\n",
            "CSV type: DFC\n",
            "Removing: downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_DFC_MI_con_2012.csv, its not in the allowed list: ['BPA', 'BPP', 'DRE', 'DVA']\n",
            "CSV type: DFC\n",
            "Removing: downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_DFC_MI_ind_2012.csv, its not in the allowed list: ['BPA', 'BPP', 'DRE', 'DVA']\n",
            "CSV type: DMPL\n",
            "Removing: downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_DMPL_con_2012.csv, its not in the allowed list: ['BPA', 'BPP', 'DRE', 'DVA']\n",
            "CSV type: DMPL\n",
            "Removing: downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_DMPL_ind_2012.csv, its not in the allowed list: ['BPA', 'BPP', 'DRE', 'DVA']\n",
            "CSV type: DRA\n",
            "Removing: downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_DRA_con_2012.csv, its not in the allowed list: ['BPA', 'BPP', 'DRE', 'DVA']\n",
            "CSV type: DRA\n",
            "Removing: downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_DRA_ind_2012.csv, its not in the allowed list: ['BPA', 'BPP', 'DRE', 'DVA']\n",
            "CSV type: DRE\n",
            "CSV type: DRE\n",
            "CSV type: DVA\n",
            "CSV type: DVA\n",
            "Removing ind files (keeping con only): downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_BPA_ind_2012.csv\n",
            "Removing ind files (keeping con only): downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_BPP_ind_2012.csv\n",
            "Removing ind files (keeping con only): downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_DRE_ind_2012.csv\n",
            "Removing ind files (keeping con only): downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_DVA_ind_2012.csv\n",
            "Converting encoding: downloaded_files/itr_cia_aberta_2012/itr_cia_aberta_BPA_con_2012.csv\n"
          ]
        }
      ],
      "source": [
        "# Function to check if a file's encoding is UTF-8\n",
        "def is_utf8(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            raw_data = file.read()\n",
        "            result = chardet.detect(raw_data)\n",
        "            return result['encoding'] == 'utf-8'\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# URL of the website containing the ZIP files\n",
        "base_url = \"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/ITR/DADOS/\"\n",
        "\n",
        "# Create a directory to save the downloaded files\n",
        "download_dir = \"downloaded_files\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# Define the years you want to download\n",
        "years_to_download = set(str(year) for year in years)\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(base_url)\n",
        "response.raise_for_status()\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Find all links on the page\n",
        "links = soup.find_all(\"a\")\n",
        "\n",
        "# Iterate through the links and download ZIP files for the specified years\n",
        "for link in links:\n",
        "    file_url = urljoin(base_url, link[\"href\"])\n",
        "\n",
        "    # Example: Assuming ZIP files are named like \"itr_cia_aberta_YEAR.zip\"\n",
        "    if file_url.endswith(\".zip\"):\n",
        "        zip_file_name = os.path.basename(file_url)\n",
        "\n",
        "        # Extract the year from the ZIP file name\n",
        "        year_part = zip_file_name.split(\"_\")[-1].split(\".\")[0]\n",
        "\n",
        "        # Check if the ZIP file is from a year within the specified range\n",
        "        if year_part in years_to_download:\n",
        "            # Check if the ZIP file already exists\n",
        "            if not os.path.exists(os.path.join(download_dir, zip_file_name)):\n",
        "                print(f\"Downloading: {zip_file_name}\")\n",
        "                with open(os.path.join(download_dir, zip_file_name), \"wb\") as file:\n",
        "                    file_response = requests.get(file_url)\n",
        "                    file.write(file_response.content)\n",
        "            else:\n",
        "                print(f\"File already exists: {zip_file_name}\")\n",
        "\n",
        "# Process the downloaded ZIP files (extract, remove \"ind\" files, and convert)\n",
        "for zip_file_name in os.listdir(download_dir):\n",
        "    if zip_file_name.endswith(\".zip\"):\n",
        "        zip_file_path = os.path.join(download_dir, zip_file_name)\n",
        "        zip_subdir = os.path.splitext(zip_file_name)[0]  # Use ZIP file name without extension as subdirectory name\n",
        "        zip_subdir_path = os.path.join(download_dir, zip_subdir)\n",
        "\n",
        "        # Check if the ZIP file has already been extracted\n",
        "        if not os.path.exists(zip_subdir_path):\n",
        "            print(f\"Unzipping: {zip_file_name} -> {zip_subdir_path}\")\n",
        "            with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "                zip_ref.extractall(zip_subdir_path)\n",
        "\n",
        "            # Remove files that are not in file_types\n",
        "            for root, _, files in os.walk(zip_subdir_path):\n",
        "                for file_name in files:\n",
        "                    csv_file_type = os.path.splitext(file_name)[0][15:19].replace(\"_\", \"\")\n",
        "                    print(f\"CSV type: {csv_file_type}\")\n",
        "                    if csv_file_type not in file_types:\n",
        "                        file_path = os.path.join(root, file_name)\n",
        "                        os.remove(file_path)\n",
        "                        print(f\"Removing: {file_path}, its not in the allowed list: {file_types}\")\n",
        "\n",
        "            # Remove files with \"ind\" in their names\n",
        "            for root, _, files in os.walk(zip_subdir_path):\n",
        "                for file_name in files:\n",
        "                    if \"ind\" in file_name:\n",
        "                        file_path = os.path.join(root, file_name)\n",
        "                        os.remove(file_path)\n",
        "                        print(f\"Removing ind files (keeping con only): {file_path}\")\n",
        "\n",
        "            # Remove files with \"itr_cia_aberta_20\" in their names\n",
        "            for root, _, files in os.walk(zip_subdir_path):\n",
        "                for file_name in files:\n",
        "                    if \"itr_cia_aberta_20\" in file_name:\n",
        "                        file_path = os.path.join(root, file_name)\n",
        "                        os.remove(file_path)\n",
        "                        print(f\"Removing itr_cia_aberta_20 file: {file_path}\")\n",
        "\n",
        "            # Convert CSV files from ISO-8859-1 to UTF-8 only if they are not already UTF-8\n",
        "            for csv_file_name in os.listdir(zip_subdir_path):\n",
        "                if csv_file_name.endswith(\".csv\"):\n",
        "                    csv_file_path = os.path.join(zip_subdir_path, csv_file_name)\n",
        "                    if not is_utf8(csv_file_path):\n",
        "                        print(f\"Converting encoding: {csv_file_path}\")\n",
        "                        with open(csv_file_path, 'r', encoding='ISO-8859-1') as source_file:\n",
        "                            content = source_file.read()\n",
        "                        with open(csv_file_path, 'w', encoding='utf-8') as target_file:\n",
        "                            target_file.write(content)\n",
        "                    else:\n",
        "                        print(f\"File already in UTF-8: {csv_file_path}\")\n",
        "\n",
        "print(\"Download, extraction, file removal, and encoding conversion completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3bda8e0",
      "metadata": {
        "id": "a3bda8e0"
      },
      "source": [
        "### Creating the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba9bc4e",
      "metadata": {
        "id": "6ba9bc4e",
        "outputId": "097fceb5-3e67-4742-953e-4533cab4242c"
      },
      "outputs": [],
      "source": [
        "bpa_df = None\n",
        "bpp_df = None\n",
        "dre_df = None\n",
        "dva_df = None\n",
        "\n",
        "d_parser = lambda x: pd.datetime.strptime(format='%Y-%m-%d')\n",
        "\n",
        "# Load files into Data Frames\n",
        "for dir_name in os.listdir(download_dir):\n",
        "    if not dir_name.endswith(\".zip\"):\n",
        "        dir_path = os.path.join(download_dir, dir_name)\n",
        "\n",
        "        for csv_file_name in os.listdir(dir_path):\n",
        "            csv_file_path = os.path.join(dir_path, csv_file_name)\n",
        "            print(\"\\n-------------------------------------------------------\")\n",
        "            print(f\"Reading csv file: {csv_file_path}\")\n",
        "\n",
        "            csv_file_type = os.path.splitext(csv_file_name)[0][15:19].replace(\"_\", \"\")\n",
        "            print(f\"CSV type: {csv_file_type}\")\n",
        "\n",
        "            local_df = pd.read_csv(csv_file_path, sep=';')\n",
        "\n",
        "            local_df['DT_REFER'] = pd.to_datetime(local_df['DT_REFER'], format='%Y-%m-%d')\n",
        "            local_df['DT_FIM_EXERC'] = pd.to_datetime(local_df['DT_FIM_EXERC'], format='%Y-%m-%d')\n",
        "\n",
        "            print(f\"Dataframe size: {len(local_df)}\")\n",
        "\n",
        "            match csv_file_type:\n",
        "                case 'BPA':\n",
        "                    if bpa_df is None:\n",
        "                        bpa_df = local_df\n",
        "                    else:\n",
        "                        bpa_df = pd.concat([bpa_df, local_df])\n",
        "                    print(f\"Dataframe size - after concat: {len(bpa_df)}\")\n",
        "\n",
        "                case 'BPP':\n",
        "                    if bpp_df is None:\n",
        "                        bpp_df = local_df\n",
        "                    else:\n",
        "                        bpp_df = pd.concat([bpp_df, local_df])\n",
        "                    print(f\"Dataframe size - after concat: {len(bpp_df)}\")\n",
        "\n",
        "                case 'DRE':\n",
        "                    if dre_df is None:\n",
        "                        dre_df = local_df\n",
        "                    else:\n",
        "                        dre_df = pd.concat([dre_df, local_df])\n",
        "                    print(f\"Dataframe size - after concat: {len(dre_df)}\")\n",
        "\n",
        "                case 'DVA':\n",
        "                    if dva_df is None:\n",
        "                        dva_df = local_df\n",
        "                    else:\n",
        "                        dva_df = pd.concat([dva_df, local_df])\n",
        "                    print(f\"Dataframe size - after concat: {len(dva_df)}\")\n",
        "\n",
        "print(\"\\nbpa_df:\")\n",
        "print(bpa_df.head(2))\n",
        "\n",
        "print(\"\\nbpp_df:\")\n",
        "print(bpp_df.head(2))\n",
        "\n",
        "print(\"\\ndre_df:\")\n",
        "print(dre_df.head(2))\n",
        "\n",
        "print(\"\\ndva_df:\")\n",
        "print(dva_df.head(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6bd694d",
      "metadata": {
        "id": "a6bd694d"
      },
      "source": [
        "### Saving Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adfaa9c2",
      "metadata": {
        "id": "adfaa9c2",
        "outputId": "d5f34075-028b-4c49-f960-a077f330c0e5"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\")\n",
        "def saveToFile(dfName, df):\n",
        "    fileName = dfName + '.csv'\n",
        "    df = df.set_index('DT_REFER')\n",
        "    print(f'\\nsaving file: {fileName}')\n",
        "    print(df.head(2))\n",
        "    df.to_csv('processed_files/' + fileName, sep=';')\n",
        "\n",
        "saveToFile('bpa_df', bpa_df)\n",
        "saveToFile('bpp_df', bpp_df)\n",
        "saveToFile('dre_df', dre_df)\n",
        "saveToFile('dva_df', dva_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b97c5d71",
      "metadata": {
        "id": "b97c5d71"
      },
      "source": [
        "## 2. Data analising"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9747abc",
      "metadata": {
        "id": "b9747abc"
      },
      "source": [
        "### Loading Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136cdef4",
      "metadata": {
        "id": "136cdef4",
        "outputId": "6b663ffc-0f1b-483f-eb9e-e5725cd4763f"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\")\n",
        "def readCsv(dfName):\n",
        "    fileName = dfName + '.csv'\n",
        "    print(f'\\nreading file: {fileName}')\n",
        "    df_read = pd.read_csv('processed_files/' + fileName, sep=';')\n",
        "    print(df_read.head(2))\n",
        "    return df_read\n",
        "\n",
        "bpa_df = readCsv('bpa_df')\n",
        "bpp_df = readCsv('bpp_df')\n",
        "dre_df = readCsv('dre_df')\n",
        "dva_df = readCsv('dva_df')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72f94bc6",
      "metadata": {
        "id": "72f94bc6"
      },
      "source": [
        "### Describing data structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adcb44d2",
      "metadata": {
        "id": "adcb44d2",
        "outputId": "d44b1c9a-c7e8-4a30-94c1-0e6ea75ed7e1"
      },
      "outputs": [],
      "source": [
        "def print_df(df_name, df):\n",
        "    print(f'\\n\\n{df_name}: {len(df):,} records')\n",
        "    print('-------------------------------------------------------------------')\n",
        "    print('\\t\\t\\t TYPES')\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print('\\t\\t\\t NULL VALUES')\n",
        "    print('CNPJ_CIA: ' + str(df['CNPJ_CIA'].isnull().sum().sum()))\n",
        "    print('DT_REFER: ' + str(df['DT_REFER'].isnull().sum().sum()))\n",
        "    print('VERSAO: ' + str(df['VERSAO'].isnull().sum().sum()))\n",
        "    print('DENOM_CIA: ' + str(df['DENOM_CIA'].isnull().sum().sum()))\n",
        "    print('CD_CVM: ' + str(df['CD_CVM'].isnull().sum().sum()))\n",
        "    print('GRUPO_DFP: ' + str(df['GRUPO_DFP'].isnull().sum().sum()))\n",
        "    print('MOEDA: ' + str(df['MOEDA'].isnull().sum().sum()))\n",
        "    print('ESCALA_MOEDA: ' + str(df['ESCALA_MOEDA'].isnull().sum().sum()))\n",
        "    print('ORDEM_EXERC: ' + str(df['ORDEM_EXERC'].isnull().sum().sum()))\n",
        "    print('DT_FIM_EXERC: ' + str(df['DT_FIM_EXERC'].isnull().sum().sum()))\n",
        "    print('CD_CONTA: ' + str(df['CD_CONTA'].isnull().sum().sum()))\n",
        "    print('DS_CONTA: ' + str(df['DS_CONTA'].isnull().sum().sum()))\n",
        "    print('VL_CONTA: ' + str(df['VL_CONTA'].isnull().sum().sum()))\n",
        "    print('ST_CONTA_FIXA: ' + str(df['ST_CONTA_FIXA'].isnull().sum().sum()))\n",
        "\n",
        "    print('\\t\\t\\t COUNT UNIQUE VALUES')\n",
        "    print('CNPJ_CIA: ' + str(len(df['CNPJ_CIA'].unique())))\n",
        "    print('DT_REFER: ' + str(len(df['DT_REFER'].unique())))\n",
        "    print('VERSAO: ' + str(len(df['VERSAO'].unique())))\n",
        "    print('DENOM_CIA: ' + str(len(df['DENOM_CIA'].unique())))\n",
        "    print('CD_CVM: ' + str(len(df['CD_CVM'].unique())))\n",
        "    print('GRUPO_DFP: ' + str(len(df['GRUPO_DFP'].unique())))\n",
        "    print('MOEDA: ' + str(len(df['MOEDA'].unique())))\n",
        "    print('ESCALA_MOEDA: ' + str(len(df['ESCALA_MOEDA'].unique())))\n",
        "    print('ORDEM_EXERC: ' + str(len(df['ORDEM_EXERC'].unique())))\n",
        "    print('DT_FIM_EXERC: ' + str(len(df['DT_FIM_EXERC'].unique())))\n",
        "    print('CD_CONTA: ' + str(len(df['CD_CONTA'].unique())))\n",
        "    print('DS_CONTA: ' + str(len(df['DS_CONTA'].unique())))\n",
        "    print('VL_CONTA: ' + str(len(df['VL_CONTA'].unique())))\n",
        "    print('ST_CONTA_FIXA: ' + str(len(df['ST_CONTA_FIXA'].unique())))\n",
        "\n",
        "    print('\\t\\t\\t UNIQUE VALUES')\n",
        "    print('CNPJ_CIA: ' + str(len(df['CNPJ_CIA'].unique())))\n",
        "    print('DT_REFER: ' + str(len(df['DT_REFER'].unique())))\n",
        "    print('VERSAO: ' + str(df['VERSAO'].unique()))\n",
        "    print('DENOM_CIA: ' + str(len(df['DENOM_CIA'].unique())))\n",
        "    print('CD_CVM: ' + str(len(df['CD_CVM'].unique())))\n",
        "    print('GRUPO_DFP: ' + str(df['GRUPO_DFP'].unique()))\n",
        "    print('MOEDA: ' + str(df['MOEDA'].unique()))\n",
        "    print('ESCALA_MOEDA: ' + str(df['ESCALA_MOEDA'].unique()))\n",
        "    print('ORDEM_EXERC: ' + str(df['ORDEM_EXERC'].unique()))\n",
        "    print('DT_FIM_EXERC: ' + str(len(df['DT_FIM_EXERC'].unique())))\n",
        "    print('CD_CONTA: ' + str(len(df['CD_CONTA'].unique())))\n",
        "    print('DS_CONTA: ' + str(len(df['DS_CONTA'].unique())))\n",
        "    print('VL_CONTA: ' + str(len(df['VL_CONTA'].unique())))\n",
        "    print('ST_CONTA_FIXA: ' + str(df['ST_CONTA_FIXA'].unique()))\n",
        "\n",
        "\n",
        "print_df('bpa_df', bpa_df)\n",
        "print_df('bpp_df', bpp_df)\n",
        "print_df('dre_df', dre_df)\n",
        "print_df('dva_df', dva_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82b514ab",
      "metadata": {
        "id": "82b514ab"
      },
      "source": [
        "## 3. Data cleansing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3383b731",
      "metadata": {
        "id": "3383b731"
      },
      "source": [
        "### Cleaning up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3959829",
      "metadata": {
        "id": "a3959829",
        "outputId": "c34b8664-a814-47e4-f041-5fe5719bcf87"
      },
      "outputs": [],
      "source": [
        "# calculate value according to field ESCALA_MOELA\n",
        "def set_vl_conta(df_name, df):\n",
        "    print(f'setting vl_conta based on escala_moeda: {df_name}')\n",
        "    df['VL_CONTA'] = np.where(df['ESCALA_MOEDA'] == 'MIL', df['VL_CONTA'] * 1000, df['VL_CONTA'])\n",
        "    return df\n",
        "\n",
        "def remove_non_mandatory_columns(df_name, df):\n",
        "    print(f'removing non mandatory columns from: {df_name}')\n",
        "    existing_cols = df.columns.intersection(['CNPJ_CIA', 'VERSAO', 'DENOM_CIA', 'MOEDA', 'ESCALA_MOEDA', 'DT_INI_EXERC', 'COLUNA_DF', 'ST_CONTA_FIXA'])\n",
        "    return df.drop(columns=existing_cols, axis=1)\n",
        "\n",
        "def remove_non_used_accounts(df_name, df):\n",
        "    print(f'removing non used accounts for: {df_name}')\n",
        "    print(f'accounts selected: {account_to_be_kept}')\n",
        "    return df[df['CD_CONTA'].isin(account_to_be_kept)]\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "bpa_df = set_vl_conta('bpa_df', bpa_df)\n",
        "bpp_df = set_vl_conta('bpp_df', bpp_df)\n",
        "dre_df = set_vl_conta('dre_df', dre_df)\n",
        "dva_df = set_vl_conta('dva_df', dva_df)\n",
        "\n",
        "print(\"\\n\")\n",
        "bpa_df = remove_non_mandatory_columns('bpa_df', bpa_df)\n",
        "bpp_df = remove_non_mandatory_columns('bpp_df', bpp_df)\n",
        "dre_df = remove_non_mandatory_columns('dre_df', dre_df)\n",
        "dva_df = remove_non_mandatory_columns('dva_df', dva_df)\n",
        "\n",
        "print(\"\\n\")\n",
        "bpa_df = remove_non_used_accounts('bpa_df', bpa_df)\n",
        "bpp_df = remove_non_used_accounts('bpp_df', bpp_df)\n",
        "dre_df = remove_non_used_accounts('dre_df', dre_df)\n",
        "dva_df = remove_non_used_accounts('dva_df', dva_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8e7c98e",
      "metadata": {
        "id": "a8e7c98e"
      },
      "source": [
        "## 4. Data transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4add67c4",
      "metadata": {
        "id": "4add67c4"
      },
      "source": [
        "### Merge dataframes into one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c03fc03f",
      "metadata": {
        "id": "c03fc03f",
        "outputId": "1bb05262-4083-449f-fc19-314e196141d1"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([bpa_df, bpp_df, dre_df, dva_df])\n",
        "\n",
        "print(df.dtypes)\n",
        "print(\"\\n\")\n",
        "print(df)\n",
        "print(\"\\n\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04803193",
      "metadata": {
        "id": "04803193"
      },
      "source": [
        "### Saving merged Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14fd408",
      "metadata": {
        "id": "b14fd408",
        "outputId": "d4d2af97-cf2b-4f78-8d13-68b4a8ca2089"
      },
      "outputs": [],
      "source": [
        "saveToFile('df', df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "076a7530",
      "metadata": {
        "id": "076a7530"
      },
      "source": [
        "### Loading merged Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e891d0ab",
      "metadata": {
        "id": "e891d0ab",
        "outputId": "e559a896-c99c-4bd1-dd03-d3486cc40d9e"
      },
      "outputs": [],
      "source": [
        "df = readCsv('df')\n",
        "\n",
        "print(df.dtypes)\n",
        "print(\"\\n\")\n",
        "print(df)\n",
        "print(\"\\n\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57047f37",
      "metadata": {
        "id": "57047f37"
      },
      "source": [
        "### Selecting companies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5678c744",
      "metadata": {
        "id": "5678c744",
        "outputId": "2e969349-a651-4ccb-981d-73a17518eb86"
      },
      "outputs": [],
      "source": [
        "print(f'selecting companies : {companies}')\n",
        "companies_df = df[df['CD_CVM'].isin(companies)]\n",
        "\n",
        "print(companies_df.dtypes)\n",
        "print(\"\\n\")\n",
        "print(companies_df)\n",
        "print(\"\\n\")\n",
        "companies_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "987dea0d",
      "metadata": {
        "id": "987dea0d"
      },
      "source": [
        "### Saving companies Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45048fa3",
      "metadata": {
        "id": "45048fa3",
        "outputId": "a17926b6-020f-41e0-e8cf-70c6a37dcaef"
      },
      "outputs": [],
      "source": [
        "saveToFile('../companies_df', companies_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7a17bb",
      "metadata": {
        "id": "6c7a17bb"
      },
      "source": [
        "### Loading companies DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fab688bb",
      "metadata": {
        "id": "fab688bb",
        "outputId": "6787b21e-88df-454d-ebb4-b10aa733a667"
      },
      "outputs": [],
      "source": [
        "df = readCsv('../companies_df')\n",
        "\n",
        "print(df.dtypes)\n",
        "print(\"\\n\")\n",
        "print(df)\n",
        "print(\"\\n\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37de4073",
      "metadata": {
        "id": "37de4073"
      },
      "source": [
        "### Basic functions for cleaning up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d79ceb5",
      "metadata": {
        "id": "3d79ceb5"
      },
      "outputs": [],
      "source": [
        "# Reorder the columns to move 'ORDEM_EXERC' to the last position\n",
        "cols = list(df.columns)\n",
        "cols.remove('ORDEM_EXERC')\n",
        "cols.insert(-1, 'ORDEM_EXERC')\n",
        "df = df[cols]\n",
        "\n",
        "# Reorder the columns to move 'CD_CONTA' to the 2nd position\n",
        "cols = list(df.columns)\n",
        "cols.remove('CD_CONTA')\n",
        "cols.insert(2, 'CD_CONTA')\n",
        "df = df[cols]\n",
        "\n",
        "# DT_REFER asdatetime\n",
        "df['DT_REFER'] = pd.to_datetime(df['DT_REFER'], format='%Y-%m-%d')\n",
        "\n",
        "\n",
        "def createPenultimo_DT_FIM_EXERC(df, position):\n",
        "\n",
        "    columnName = 'DT_FIM_EXERC'\n",
        "\n",
        "    # The field DT_FIM_EXERC for BPA and BPP is 3 months in the past, for the others (DRE, DVA) this value is past 12 months\n",
        "    newColumnNameBP = columnName + '_PENULTIMO' + '_BP'\n",
        "    newColumnName = columnName + '_PENULTIMO'\n",
        "    bpList = ['DF Consolidado - Balanço Patrimonial Ativo', 'DF Consolidado - Balanço Patrimonial Passivo']\n",
        "\n",
        "    df[newColumnNameBP] = np.where(\n",
        "        (df['ORDEM_EXERC'] == 'PENÚLTIMO') & df['GRUPO_DFP'].isin(bpList),\n",
        "        df[columnName],\n",
        "        None)\n",
        "    df[newColumnName] = np.where(\n",
        "        (df['ORDEM_EXERC'] == 'PENÚLTIMO') & ~(df['GRUPO_DFP'].isin(bpList)),\n",
        "        df[columnName],\n",
        "        None)\n",
        "\n",
        "    # remove this same values from column\n",
        "    df[columnName] = np.where(\n",
        "        df['ORDEM_EXERC'] != 'PENÚLTIMO',\n",
        "        df[columnName],\n",
        "        None)\n",
        "\n",
        "    df[newColumnNameBP] = pd.to_datetime(df[newColumnNameBP], format='%Y-%m-%d')\n",
        "    df[newColumnName] = pd.to_datetime(df[newColumnName], format='%Y-%m-%d')\n",
        "    df[columnName] = pd.to_datetime(df[columnName], format='%Y-%m-%d')\n",
        "\n",
        "    # Reorder the columns to move to correct position\n",
        "    cols = list(df.columns)\n",
        "\n",
        "        # BP\n",
        "    cols.remove(newColumnNameBP)\n",
        "    cols.insert(position, newColumnNameBP)\n",
        "    df = df[cols]\n",
        "\n",
        "        # not BP\n",
        "    cols.remove(newColumnName)\n",
        "    cols.insert(position+1, newColumnName)\n",
        "    df = df[cols]\n",
        "\n",
        "    return df\n",
        "\n",
        "def createPenultimo_VL_CONTA(df, position):\n",
        "\n",
        "    columnName = 'VL_CONTA'\n",
        "\n",
        "    # The field DT_FIM_EXERC for BPA and BPP is 3 months in the past, for the others (DRE, DVA) this value is past 12 months\n",
        "    newColumnName = columnName + '_PENULTIMO'\n",
        "\n",
        "    df[newColumnName] = np.where(\n",
        "        (df['ORDEM_EXERC'] == 'PENÚLTIMO'),\n",
        "        df[columnName],\n",
        "        None)\n",
        "\n",
        "    # remove this same values from column\n",
        "    df[columnName] = np.where(\n",
        "        df['ORDEM_EXERC'] != 'PENÚLTIMO',\n",
        "        df[columnName],\n",
        "        None)\n",
        "\n",
        "    df[newColumnName] = df[newColumnName].astype(float)\n",
        "    df[columnName] = df[columnName].astype(float)\n",
        "\n",
        "    # Reorder the columns to move to correct position\n",
        "    cols = list(df.columns)\n",
        "\n",
        "    cols.remove(newColumnName)\n",
        "    cols.insert(position, newColumnName)\n",
        "    df = df[cols]\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d0c2549",
      "metadata": {
        "id": "6d0c2549"
      },
      "source": [
        "### Create column 'DT_FIM_EXERC_PENULTIMO'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc32e6f4",
      "metadata": {
        "id": "bc32e6f4",
        "outputId": "8abba713-d98e-45bf-b4ea-e031933d41cc"
      },
      "outputs": [],
      "source": [
        "df = createPenultimo_DT_FIM_EXERC(df, 4)\n",
        "\n",
        "print(df.dtypes)\n",
        "print(\"\\n\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1b915e",
      "metadata": {
        "id": "1b1b915e"
      },
      "source": [
        "#### Create column 'VL_CONTA_PENULTIMO'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86430632",
      "metadata": {
        "id": "86430632",
        "outputId": "02b06a6a-2e54-4527-dd57-e71307b21271"
      },
      "outputs": [],
      "source": [
        "df = createPenultimo_VL_CONTA(df, 8)\n",
        "\n",
        "print(df.dtypes)\n",
        "print(\"\\n\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73545271",
      "metadata": {
        "id": "73545271"
      },
      "source": [
        "#### Remove disposable columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53164a6a",
      "metadata": {
        "id": "53164a6a",
        "outputId": "f90a8de7-bc91-4a8c-feee-78908a84f97f"
      },
      "outputs": [],
      "source": [
        "cols = df.columns.intersection(['GRUPO_DFP', 'DS_CONTA', 'ORDEM_EXERC'])\n",
        "df = df.drop(columns=cols, axis=1)\n",
        "\n",
        "print(df.dtypes)\n",
        "print(\"\\n\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e82ab015",
      "metadata": {
        "id": "e82ab015"
      },
      "source": [
        "#### Group by 'DT_REFER', 'CD_CVM', and 'CD_CONTA' and aggregate the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "307ec0f2",
      "metadata": {
        "id": "307ec0f2",
        "outputId": "f3c622ef-df95-41fa-ad8a-1e72964e032b"
      },
      "outputs": [],
      "source": [
        "agg_funcs = {\n",
        "    'DT_FIM_EXERC_PENULTIMO_BP': 'first',\n",
        "    'DT_FIM_EXERC_PENULTIMO': 'first',\n",
        "    'DT_FIM_EXERC': 'first',\n",
        "    'VL_CONTA_PENULTIMO': 'last',\n",
        "    'VL_CONTA': 'last'\n",
        "}\n",
        "\n",
        "df = df.groupby(['DT_REFER', 'CD_CVM', 'CD_CONTA']).agg(agg_funcs).reset_index()\n",
        "\n",
        "# Reorder the columns as needed\n",
        "df = df[['DT_REFER', 'CD_CVM', 'CD_CONTA', 'DT_FIM_EXERC_PENULTIMO_BP', 'DT_FIM_EXERC_PENULTIMO', 'DT_FIM_EXERC', 'VL_CONTA_PENULTIMO', 'VL_CONTA']]\n",
        "\n",
        "# View the final DataFrame\n",
        "print(df.dtypes)\n",
        "print(\"\\n\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea3d1e5c",
      "metadata": {
        "id": "ea3d1e5c"
      },
      "source": [
        "#### Saving df before transposing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2fb83a3",
      "metadata": {
        "id": "c2fb83a3",
        "outputId": "e7fc6922-19b7-46e4-cf5c-323221176be5"
      },
      "outputs": [],
      "source": [
        "saveToFile('../companies_before_transposing_df', df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c8c985",
      "metadata": {
        "id": "f7c8c985"
      },
      "source": [
        "#### Loading df before transposing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8503aa7",
      "metadata": {
        "id": "a8503aa7",
        "outputId": "e21e7e10-077f-4471-8c17-c3bf8c3a9848"
      },
      "outputs": [],
      "source": [
        "df = readCsv('../companies_before_transposing_df')\n",
        "\n",
        "print(df.dtypes)\n",
        "print(\"\\n\")\n",
        "print(df)\n",
        "print(\"\\n\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f8d9b6",
      "metadata": {
        "id": "d5f8d9b6"
      },
      "source": [
        "#### Transforming rows into column to have one register by date, company and account type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f0d8d46",
      "metadata": {
        "id": "5f0d8d46",
        "outputId": "9dcf8a7f-3484-4629-a3b1-b03f0af393ad"
      },
      "outputs": [],
      "source": [
        "# Filling NaN values with empty strings\n",
        "df.fillna('', inplace=True)\n",
        "\n",
        "# Creating the pivot table\n",
        "df_pivot = df.pivot_table(\n",
        "    index=['DT_REFER', 'CD_CVM', 'DT_FIM_EXERC_PENULTIMO_BP', 'DT_FIM_EXERC_PENULTIMO', 'DT_FIM_EXERC'],\n",
        "    columns='CD_CONTA',\n",
        "    values=['VL_CONTA_PENULTIMO', 'VL_CONTA'],\n",
        "    aggfunc='first'\n",
        ").reset_index()\n",
        "\n",
        "# Flattening the multi-level columns\n",
        "df_pivot.columns = ['_'.join(col).strip() if col[1] else col[0] for col in df_pivot.columns.values]\n",
        "\n",
        "# Reordering the columns\n",
        "ordered_columns = (\n",
        "    ['DT_REFER', 'CD_CVM', 'DT_FIM_EXERC_PENULTIMO_BP', 'DT_FIM_EXERC_PENULTIMO', 'DT_FIM_EXERC'] +\n",
        "    [col for col in df_pivot.columns if col.startswith('VL_CONTA_PENULTIMO_')] +\n",
        "    [col for col in df_pivot.columns if col.startswith('VL_CONTA_') and not col.startswith('VL_CONTA_PENULTIMO_')]\n",
        ")\n",
        "\n",
        "df_pivot = df_pivot[ordered_columns]\n",
        "\n",
        "print(\"\\n-------------------------------------\")\n",
        "print(df_pivot)\n",
        "print(\"-------------------------------------\\n\")\n",
        "\n",
        "def custom_agg(series):\n",
        "    # If the dtype is datetime, check for non-NaT values\n",
        "    if series.dtype == 'datetime64[ns]':\n",
        "        non_nat_values = [val for val in series if not pd.isna(val)]\n",
        "        return non_nat_values[0] if non_nat_values else pd.NaT\n",
        "    else:\n",
        "        # If it's numeric, check for non-NaN values\n",
        "        if pd.api.types.is_numeric_dtype(series):\n",
        "            non_nan_values = [val for val in series if not pd.isna(val)]\n",
        "            return non_nan_values[0] if non_nan_values else np.nan\n",
        "        # Else check for non-empty strings\n",
        "        else:\n",
        "            non_empty_values = [val for val in series if val != '']\n",
        "            return non_empty_values[0] if non_empty_values else ''\n",
        "\n",
        "# Create an aggregation dictionary excluding columns you're grouping by\n",
        "grouping_columns = ['DT_REFER', 'CD_CVM', 'DT_FIM_EXERC']\n",
        "agg_dict = {col: custom_agg for col in ordered_columns if col not in grouping_columns}\n",
        "\n",
        "# Group by 'DT_REFER', 'CD_CVM', and 'DT_FIM_EXERC', then aggregate\n",
        "grouped_df = df_pivot.groupby(grouping_columns).agg(agg_dict).reset_index()\n",
        "\n",
        "cols = list(grouped_df.columns)\n",
        "cols.remove('DT_FIM_EXERC')\n",
        "cols.insert(4, 'DT_FIM_EXERC')\n",
        "grouped_df = grouped_df[cols]\n",
        "\n",
        "print(\"\\n-------------------------------------\")\n",
        "print(grouped_df)\n",
        "print(\"-------------------------------------\\n\")\n",
        "\n",
        "saveToFile('../df_pivot', grouped_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53e08a98",
      "metadata": {},
      "source": [
        "#### Reading the cleaned up and grouped dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b7aa66",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df = readCsv('../df_pivot')\n",
        "\n",
        "print(final_df.dtypes)\n",
        "print(\"\\n\")\n",
        "print(final_df)\n",
        "print(\"\\n\")\n",
        "final_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21125d52",
      "metadata": {
        "id": "21125d52"
      },
      "source": [
        "#### Free Cash Flow formula\n",
        "\n",
        "*FCFE = <br>\n",
        "  3.11 (Lucro/Prejuizo Consolidado do Periodo do DRE)<br>\n",
        "  \\- (Δ 1.02.03 (Imobilizado) - 7.04.01 (Depreciacao, Amortizacao e Exaustao da DVA))<br>\n",
        "  \\- (Δ 1.01 (Ativo Circulante) - Δ 1.01.01 (Caixa e Equivalentes de Caixa) -  Δ 2.01 (Passivo Circulante))<br>\n",
        "  \\+ (Δ 2.01.04(Emprestimos e Financiamentos Circulantes) + Δ 2.02.01(Emprestimos e Financiamentos Nao Circulantes))*<br><br>\n",
        "\n",
        "\n",
        "*3.11 (Lucro/Prejuizo Consolidado do Periodo do DRE) = <br>\n",
        "  LUCRO_LIQUIDO_PERIODO*<br>\n",
        "\n",
        "\n",
        "*Δ 1.02.03 (Imobilizado) = <br>\n",
        "  IMOBILIZADO_NO_PERIODO_ANTERIOR - IMOBILIZADO_NO_PERIODO*<br>\n",
        "\n",
        "\n",
        "*7.04.01 (Depreciacao, Amortizacao e Exaustao da DVA) = <br>\n",
        "  DEPRECIACAO_AMORTIZACAO_E_EXAUSTAO_DA_DVA*<br>\n",
        "\n",
        "\n",
        "*Δ 1.01 (Ativo Circulante) = <br>\n",
        "  ATIVO_CIRCULANTE_NO_PERIODO_ANTERIOR - ATIVO_CIRCULANTE_NO_PERIODO*<br>\n",
        "\n",
        "\n",
        "*Δ 1.01.01 (Caixa e Equivalentes de Caixa) = <br>\n",
        "  CAIXA_E_EQUIVALENTES_DE_CAIXA_NO_PERIODO_ANTERIOR - CAIXA_E_EQUIVALENTES_DE_CAIXA_NO_PERIODO*<br>\n",
        "\n",
        "\n",
        "*Δ 2.01 (Passivo Circulante) = <br>\n",
        "  PASSIVO_CIRCULANTE_NO_PERIODO_ANTERIOR - PASSIVO_CIRCULANTE_NO_PERIODO*<br>\n",
        "\n",
        "\n",
        "*Δ 2.01.04(Emprestimos e Financiamentos Circulantes) = <br>\n",
        "  EMPRESTIMOS_E_FINANCIAMENTOS_CIRCULANTES_NO_PERIODO_ANTERIOR - EMPRESTIMOS_E_FINANCIAMENTOS_CIRCULANTES_NO_PERIODO*<br>\n",
        "\n",
        "\n",
        "*Δ 2.02.01(Emprestimos e Financiamentos Nao Circulantes) = <br>\n",
        "  EMPRESTIMOS_E_FINANCIAMENTOS_NAO_CIRCULANTES_NO_PERIODO_ANTERIOR - EMPRESTIMOS_E_FINANCIAMENTOS_NAO_CIRCULANTES_NO_PERIODO*<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdf146b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "####################################################################\n",
        "# Lucro/Prejuizo Consolidado do Periodo do DRE (3.11)\n",
        "####################################################################\n",
        "final_df['LUCRO_LIQUIDO_PERIODO'] = final_df['VL_CONTA_3.11']\n",
        "\n",
        "\n",
        "####################################################################\n",
        "# Δ Imobilizado (1.02.03)\n",
        "####################################################################\n",
        "final_df['IMOBILIZADO_NO_PERIODO_ANTERIOR'] = final_df['VL_CONTA_PENULTIMO_1.02.03']\n",
        "final_df['IMOBILIZADO_NO_PERIODO'] = final_df['VL_CONTA_1.02.03']\n",
        "final_df['IMOBILIZADO'] = final_df['IMOBILIZADO_NO_PERIODO_ANTERIOR'] - final_df['IMOBILIZADO_NO_PERIODO']\n",
        "\n",
        "\n",
        "#################################################################### \n",
        "# Depreciacao, Amortizacao e Exaustao da DVA (7.04.01)\n",
        "#################################################################### \n",
        "final_df['DEPRECIACAO_AMORTIZACAO_E_EXAUSTAO_DA_DVA'] = final_df['VL_CONTA_7.04.01']\n",
        "\n",
        "\n",
        "#################################################################### \n",
        "# Δ Ativo Circulante (1.01)\n",
        "#################################################################### \n",
        "final_df['ATIVO_CIRCULANTE_NO_PERIODO_ANTERIOR'] = final_df['VL_CONTA_PENULTIMO_1.01']\n",
        "final_df['ATIVO_CIRCULANTE_NO_PERIODO'] = final_df['VL_CONTA_1.01']\n",
        "final_df['ATIVO_CIRCULANTE'] = final_df['ATIVO_CIRCULANTE_NO_PERIODO_ANTERIOR'] - final_df['ATIVO_CIRCULANTE_NO_PERIODO']\n",
        "\n",
        "\n",
        "#################################################################### \n",
        "# Δ Caixa e Equivalentes de Caixa (1.01.01)\n",
        "#################################################################### \n",
        "final_df['CAIXA_E_EQUIVALENTES_DE_CAIXA_NO_PERIODO_ANTERIOR'] = final_df['VL_CONTA_PENULTIMO_1.01.01']\n",
        "final_df['CAIXA_E_EQUIVALENTES_DE_CAIXA_NO_PERIODO'] = final_df['VL_CONTA_1.01.01']\n",
        "final_df['CAIXA_E_EQUIVALENTES_DE_CAIXA'] = final_df['CAIXA_E_EQUIVALENTES_DE_CAIXA_NO_PERIODO_ANTERIOR'] - final_df['CAIXA_E_EQUIVALENTES_DE_CAIXA_NO_PERIODO']\n",
        "\n",
        "\n",
        "#################################################################### \n",
        "# Δ Passivo Circulante (2.01)\n",
        "#################################################################### \n",
        "final_df['EMPRESTIMOS_E_FINANCIAMENTOS_CIRCULANTES_NO_PERIODO_ANTERIOR'] = final_df['VL_CONTA_PENULTIMO_2.01']\n",
        "final_df['EMPRESTIMOS_E_FINANCIAMENTOS_CIRCULANTES_NO_PERIODO'] = final_df['VL_CONTA_2.01']\n",
        "final_df['EMPRESTIMOS_E_FINANCIAMENTOS_CIRCULANTES'] = final_df['EMPRESTIMOS_E_FINANCIAMENTOS_CIRCULANTES_NO_PERIODO_ANTERIOR'] - final_df['EMPRESTIMOS_E_FINANCIAMENTOS_CIRCULANTES_NO_PERIODO']\n",
        "\n",
        "\n",
        "#################################################################### \n",
        "# Δ Emprestimos e Financiamentos Circulantes (2.01.04)\n",
        "#################################################################### \n",
        "final_df['PASSIVO_CIRCULANTE_NO_PERIODO_ANTERIOR'] = final_df['VL_CONTA_PENULTIMO_2.01.04']\n",
        "final_df['PASSIVO_CIRCULANTE_NO_PERIODO'] = final_df['VL_CONTA_2.01.04']\n",
        "final_df['PASSIVO_CIRCULANTE'] = final_df['PASSIVO_CIRCULANTE_NO_PERIODO_ANTERIOR'] - final_df['PASSIVO_CIRCULANTE_NO_PERIODO']\n",
        "\n",
        "\n",
        "#################################################################### \n",
        "# Δ Emprestimos e Financiamentos Nao Circulantes (2.02.01)\n",
        "#################################################################### \n",
        "final_df['EMPRESTIMOS_E_FINANCIAMENTOS_NAO_CIRCULANTES_NO_PERIODO_ANTERIOR'] = final_df['VL_CONTA_PENULTIMO_2.02.01']\n",
        "final_df['EMPRESTIMOS_E_FINANCIAMENTOS_NAO_CIRCULANTES_NO_PERIODO'] = final_df['VL_CONTA_2.02.01']\n",
        "final_df['EMPRESTIMOS_E_FINANCIAMENTOS_NAO_CIRCULANTES'] = final_df['EMPRESTIMOS_E_FINANCIAMENTOS_NAO_CIRCULANTES_NO_PERIODO_ANTERIOR'] - final_df['EMPRESTIMOS_E_FINANCIAMENTOS_NAO_CIRCULANTES_NO_PERIODO']\n",
        "\n",
        "\n",
        "# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "#\n",
        "# Fluxo de Caixa Livre para os Acionistas (Free Cash Flows to Equity) - FCFE\n",
        "#\n",
        "# $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "final_df['FCFE'] = \\\n",
        "    final_df['LUCRO_LIQUIDO_PERIODO'] \\\n",
        "    - ( final_df['IMOBILIZADO'] - final_df['DEPRECIACAO_AMORTIZACAO_E_EXAUSTAO_DA_DVA'] ) \\\n",
        "    - ( final_df['ATIVO_CIRCULANTE'] - final_df['CAIXA_E_EQUIVALENTES_DE_CAIXA'] - final_df['EMPRESTIMOS_E_FINANCIAMENTOS_CIRCULANTES'] ) \\\n",
        "    + ( final_df['PASSIVO_CIRCULANTE'] + final_df['CAIXA_E_EQUIVALENTES_DE_CAIXA'] - final_df['EMPRESTIMOS_E_FINANCIAMENTOS_NAO_CIRCULANTES'] )\n",
        "\n",
        "saveToFile('../final_df', final_df)\n",
        "\n",
        "print(final_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce9afba",
      "metadata": {},
      "source": [
        "#### Loading the data already strutured and cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f54e7397",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ARIMA\n",
        "%pip install statsmodels\n",
        "# matplotlib (para visualizações)\n",
        "%pip install matplotlib\n",
        "# pmdarima (para a seleção automática do modelo ARIMA)\n",
        "%pip install pmdarima\n",
        "# statsmodels\n",
        "%pip install statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ecea1e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('final_df.csv', sep=';')\n",
        "\n",
        "data['DT_REFER'] = pd.to_datetime(data['DT_REFER'], format='%Y-%m-%d')\n",
        "data['DT_FIM_EXERC_PENULTIMO_BP'] = pd.to_datetime(data['DT_FIM_EXERC_PENULTIMO_BP'], format='%Y-%m-%d')\n",
        "data['DT_FIM_EXERC_PENULTIMO'] = pd.to_datetime(data['DT_FIM_EXERC_PENULTIMO'], format='%Y-%m-%d')\n",
        "data['DT_FIM_EXERC'] = pd.to_datetime(data['DT_FIM_EXERC'], format='%Y-%m-%d')\n",
        "data = data.set_index('DT_REFER')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c84319",
      "metadata": {},
      "source": [
        "#### Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc529c21",
      "metadata": {},
      "source": [
        "##### Time Series Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ff55bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "def thousands_formatter(x, pos):\n",
        "    return f'{int(x):,}'\n",
        "\n",
        "formatter = FuncFormatter(thousands_formatter)\n",
        "\n",
        "def print_serie(df, company_code):\n",
        "    company = companies[company_code]\n",
        "    plt.figure(figsize=(14,7))\n",
        "    plt.plot(df.index, df['FCFE'])\n",
        "    plt.title(f\"Fluxo de Caixa Livre ao Longo do Tempo para a empresa: {company['name']} ({company['ticker']})\")\n",
        "    plt.xlabel('Data')\n",
        "    plt.ylabel('Fluxo de Caixa Livre')\n",
        "    plt.grid(True)\n",
        "    plt.ticklabel_format(style='plain', axis='y')\n",
        "    ax = plt.gca()  # get current axis\n",
        "    ax.yaxis.set_major_formatter(formatter)  # set the formatter for y axis\n",
        "    plt.show()\n",
        "\n",
        "for company_code in companies:\n",
        "    company_data = data[data['CD_CVM'] == company_code]\n",
        "    print_serie(company_data, company_code)\n",
        "\n",
        "\n",
        "def plot_combined_series(data, companies):\n",
        "    plt.figure(figsize=(14,7))\n",
        "    \n",
        "    for company_code in companies:\n",
        "        company_data = data[data['CD_CVM'] == company_code]\n",
        "        company = companies[company_code]\n",
        "        plt.plot(company_data.index, company_data['FCFE'], label=f\"{company['name']} ({company['ticker']})\")\n",
        "    \n",
        "    plt.title('Fluxo de Caixa Livre ao Longo do Tempo')\n",
        "    plt.xlabel('Data')\n",
        "    plt.ylabel('Fluxo de Caixa Livre')\n",
        "    plt.legend(loc='best')\n",
        "    plt.ticklabel_format(style='plain', axis='y')\n",
        "    ax = plt.gca()  # get current axis\n",
        "    ax.yaxis.set_major_formatter(formatter)  # set the formatter for y axis\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_combined_series(data, companies)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "400a89ee",
      "metadata": {},
      "source": [
        "##### Time Series Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1275681b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def decompose_series(data, period=12):\n",
        "    decomposition = sm.tsa.seasonal_decompose(data, period=period)\n",
        "    return decomposition\n",
        "\n",
        "def plot_individual_decomposition(data, company_code, period=12):\n",
        "    company = companies[company_code]\n",
        "    company_data = data[data['CD_CVM'] == company_code]\n",
        "\n",
        "    decomposition = decompose_series(company_data['FCFE'], period)\n",
        "\n",
        "    # Thousand separator\n",
        "    formatter = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n",
        "\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(decomposition.trend)\n",
        "    plt.title(f\"Tendência - {company['name']} ({company['ticker']})\")\n",
        "    plt.gca().yaxis.set_major_formatter(formatter)\n",
        "\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(decomposition.seasonal)\n",
        "    plt.title(f\"Sazonalidade - {company['name']} ({company['ticker']})\")\n",
        "    plt.gca().yaxis.set_major_formatter(formatter)\n",
        "\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(decomposition.resid)\n",
        "    plt.title(f\"Resíduo - {company['name']} ({company['ticker']})\")\n",
        "    plt.gca().yaxis.set_major_formatter(formatter)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for company_code in companies:\n",
        "    plot_individual_decomposition(data, company_code)\n",
        "\n",
        "\n",
        "\n",
        "def plot_grouped_decomposition(data, companies, period=12):\n",
        "    plt.figure(figsize=(14, 21))\n",
        "\n",
        "    # Thousand separator\n",
        "    formatter = ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n",
        "\n",
        "    for company_code in companies:\n",
        "        company_data = data[data['CD_CVM'] == company_code]\n",
        "        company = companies[company_code]\n",
        "\n",
        "        # Serie decomposition\n",
        "        decomposition = decompose_series(company_data['FCFE'], period)\n",
        "\n",
        "        # Plotting Tendency\n",
        "        plt.subplot(3, 1, 1)\n",
        "        plt.plot(decomposition.trend, label=f\"{company['name']} ({company['ticker']})\")\n",
        "        plt.title('Tendência Agrupada')\n",
        "        plt.legend(loc='best')\n",
        "        plt.gca().yaxis.set_major_formatter(formatter)\n",
        "\n",
        "        # Plotting Tendency Seasonality\n",
        "        plt.subplot(3, 1, 2)\n",
        "        plt.plot(decomposition.seasonal, label=f\"{company['name']} ({company['ticker']})\")\n",
        "        plt.title('Sazonalidade Agrupada')\n",
        "        plt.legend(loc='best')\n",
        "        plt.gca().yaxis.set_major_formatter(formatter)\n",
        "\n",
        "        # Plotting Resid\n",
        "        plt.subplot(3, 1, 3)\n",
        "        plt.plot(decomposition.resid, label=f\"{company['name']} ({company['ticker']})\")\n",
        "        plt.title('Resíduo Agrupado')\n",
        "        plt.legend(loc='best')\n",
        "        plt.gca().yaxis.set_major_formatter(formatter)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_grouped_decomposition(data, companies)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d41dbcd",
      "metadata": {},
      "source": [
        "##### Stationarity validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17173bb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "def adfuller_test_and_check(data):\n",
        "    # Performing the Augmented Dickey-Fuller test:\n",
        "    result = adfuller(data)\n",
        "    \n",
        "    # Printing results:\n",
        "    print('ADF Statistic:', result[0])\n",
        "    print('p-value:', result[1])\n",
        "    print('Critical Values:', result[4])\n",
        "    \n",
        "    # Checking the p-value to decide on stationarity:\n",
        "    if result[1] <= 0.05:\n",
        "        print(\"Com 95% de confiança, podemos dizer que a série é estacionária.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Com 95% de confiança, podemos dizer que a série NÃO é estacionária.\")\n",
        "        return False\n",
        "\n",
        "\n",
        "stationary_series_dict = {}\n",
        "non_stationary_companies = {}\n",
        "\n",
        "for company_code in companies:\n",
        "    company_info = companies[company_code]\n",
        "    company_data = data[data['CD_CVM'] == company_code]\n",
        "    print(\"\\n\\n------------------------------------------------------\")\n",
        "    print(f\"Resultado para a empresa {company_info['name']} ({company_info['ticker']})\")\n",
        "    print(\"------------------------------------------------------\")\n",
        "    is_stationary = adfuller_test_and_check(company_data['FCFE'])\n",
        "    if is_stationary:\n",
        "        stationary_series_dict[company_code] = company_data['FCFE']\n",
        "    else:\n",
        "        non_stationary_companies[company_code] = company_data['FCFE']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d12075c0",
      "metadata": {},
      "source": [
        "##### Non-stationarity transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab9e4221",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_series_stationary(series, max_diff=2):\n",
        "    diff_count = 0\n",
        "    stationary_series = series.copy()\n",
        "    \n",
        "    while diff_count < max_diff:\n",
        "        diff_count += 1\n",
        "        stationary_series = stationary_series.diff().dropna()  # Differencing the series\n",
        "        \n",
        "        print(\"\\n...............................................\")\n",
        "        print(\"Checking stationarity after differentiation\")\n",
        "        print(\"...............................................\")\n",
        "        is_stationary = adfuller_test_and_check(stationary_series)\n",
        "\n",
        "        if is_stationary:\n",
        "            print(f\"\\n=> Série tornou-se estacionária após {diff_count} diferenciação(ões).\")\n",
        "            return stationary_series\n",
        "        \n",
        "    print(f\"\\n=> A série não se tornou estacionária após {max_diff} diferenciações.\")\n",
        "    return None  # Returns None if unable to make the series stationary after max_diff attempts\n",
        "\n",
        "# Using the non_stationary_companies dictionary to transform the non-stationary series\n",
        "for company_code in non_stationary_companies:\n",
        "    company_info = companies[company_code]\n",
        "    company_data = data[data['CD_CVM'] == company_code]\n",
        "    print(\"\\n\\n------------------------------------------------------\")\n",
        "    print(f\"Transformando série para a empresa {company_info['name']} ({company_info['ticker']})\")\n",
        "    print(\"------------------------------------------------------\")\n",
        "    \n",
        "    # Making the series stationary\n",
        "    stationary_series = make_series_stationary(company_data['FCFE'])\n",
        "    if stationary_series is not None:\n",
        "        stationary_series_dict[company_code] = stationary_series\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89fb42d5",
      "metadata": {},
      "source": [
        "#### Prediction using ARIMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aadeb7aa",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

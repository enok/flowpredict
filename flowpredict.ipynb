{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30479985",
   "metadata": {
    "id": "30479985"
   },
   "source": [
    "#### Downloading stock market data from the CVM website.\n",
    "#### The data will consist of quarterly reports spanning from 2012 to 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03bf973",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/data/google-drive/cursos/usp/mba/data-science/tcc/code/flowpredict/flowpredict.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data/google-drive/cursos/usp/mba/data-science/tcc/code/flowpredict/flowpredict.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#pd.set_option('display.max_columns', 10)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/data/google-drive/cursos/usp/mba/data-science/tcc/code/flowpredict/flowpredict.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pd\u001b[39m.\u001b[39mreset_option(\u001b[39m'\u001b[39m\u001b[39mdisplay.max_columns\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/data/google-drive/cursos/usp/mba/data-science/tcc/code/flowpredict/flowpredict.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pd\u001b[39m.\u001b[39mset_option(\u001b[39m'\u001b[39m\u001b[39mdisplay.width\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m200\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#pd.set_option('display.max_columns', 10)\n",
    "pd.reset_option('display.max_columns')\n",
    "pd.set_option('display.width', 200)\n",
    "#pd.reset_option('display.width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22943e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4540,
     "status": "ok",
     "timestamp": 1693529355133,
     "user": {
      "displayName": "Enok Antônio de Jesus",
      "userId": "15348291901420349452"
     },
     "user_tz": 180
    },
    "id": "7a22943e",
    "outputId": "0cc9306a-7982-4423-c4c5-75b9224324db"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# install libs\n",
    "#\n",
    "%pip install requests beautifulsoup4\n",
    "%pip install chardet\n",
    "%pip install files\n",
    "%pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563bd89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135343,
     "status": "ok",
     "timestamp": 1693529493588,
     "user": {
      "displayName": "Enok Antônio de Jesus",
      "userId": "15348291901420349452"
     },
     "user_tz": 180
    },
    "id": "6563bd89",
    "outputId": "b2c3d71e-27dc-4d52-ace6-755b7f4f6077"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# download balance sheets files\n",
    "#\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import zipfile\n",
    "import chardet\n",
    "\n",
    "# Function to check if a file's encoding is UTF-8\n",
    "def is_utf8(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            raw_data = file.read()\n",
    "            result = chardet.detect(raw_data)\n",
    "            return result['encoding'] == 'utf-8'\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# URL of the website containing the ZIP files\n",
    "base_url = \"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/ITR/DADOS/\"\n",
    "\n",
    "# Create a directory to save the downloaded files\n",
    "download_dir = \"downloaded_files\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Define the years you want to download (2012 to 2022)\n",
    "years_to_download = set(str(year) for year in range(2012, 2023))\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(base_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all links on the page\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "# Iterate through the links and download ZIP files for the specified years\n",
    "for link in links:\n",
    "    file_url = urljoin(base_url, link[\"href\"])\n",
    "    \n",
    "    # Example: Assuming ZIP files are named like \"itr_cia_aberta_YEAR.zip\"\n",
    "    if file_url.endswith(\".zip\"):\n",
    "        zip_file_name = os.path.basename(file_url)\n",
    "        \n",
    "        # Extract the year from the ZIP file name\n",
    "        year_part = zip_file_name.split(\"_\")[-1].split(\".\")[0]\n",
    "        \n",
    "        # Check if the ZIP file is from a year within the specified range\n",
    "        if year_part in years_to_download:\n",
    "            # Check if the ZIP file already exists\n",
    "            if not os.path.exists(os.path.join(download_dir, zip_file_name)):\n",
    "                print(f\"Downloading: {zip_file_name}\")\n",
    "                with open(os.path.join(download_dir, zip_file_name), \"wb\") as file:\n",
    "                    file_response = requests.get(file_url)\n",
    "                    file.write(file_response.content)\n",
    "            else:\n",
    "                print(f\"File already exists: {zip_file_name}\")\n",
    "\n",
    "# Process the downloaded ZIP files (extract, remove \"ind\" files, and convert)\n",
    "for zip_file_name in os.listdir(download_dir):\n",
    "    if zip_file_name.endswith(\".zip\"):\n",
    "        zip_file_path = os.path.join(download_dir, zip_file_name)\n",
    "        zip_subdir = os.path.splitext(zip_file_name)[0]  # Use ZIP file name without extension as subdirectory name\n",
    "        zip_subdir_path = os.path.join(download_dir, zip_subdir)\n",
    "\n",
    "        # Check if the ZIP file has already been extracted\n",
    "        if not os.path.exists(zip_subdir_path):\n",
    "            print(f\"Unzipping: {zip_file_name} -> {zip_subdir_path}\")\n",
    "            with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(zip_subdir_path)\n",
    "\n",
    "            # Remove files with \"ind\" in their names\n",
    "            for root, _, files in os.walk(zip_subdir_path):\n",
    "                for file_name in files:\n",
    "                    if \"ind\" in file_name:\n",
    "                        file_path = os.path.join(root, file_name)\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Removed: {file_path}\")\n",
    "\n",
    "            # Remove files with \"itr_cia_aberta_20\" in their names\n",
    "            for root, _, files in os.walk(zip_subdir_path):\n",
    "                for file_name in files:\n",
    "                    if \"itr_cia_aberta_20\" in file_name:\n",
    "                        file_path = os.path.join(root, file_name)\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Removed: {file_path}\")\n",
    "\n",
    "            # Remove files with \"itr_cia_aberta_DFC_MD_con_20\" in their names\n",
    "            for root, _, files in os.walk(zip_subdir_path):\n",
    "                for file_name in files:\n",
    "                    if \"itr_cia_aberta_DFC_MD_con_20\" in file_name:\n",
    "                        file_path = os.path.join(root, file_name)\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Removed: {file_path}\")\n",
    "\n",
    "            # Convert CSV files from ISO-8859-1 to UTF-8 only if they are not already UTF-8\n",
    "            for csv_file_name in os.listdir(zip_subdir_path):\n",
    "                if csv_file_name.endswith(\".csv\"):\n",
    "                    csv_file_path = os.path.join(zip_subdir_path, csv_file_name)\n",
    "                    if not is_utf8(csv_file_path):\n",
    "                        print(f\"Converting encoding: {csv_file_path}\")\n",
    "                        with open(csv_file_path, 'r', encoding='ISO-8859-1') as source_file:\n",
    "                            content = source_file.read()\n",
    "                        with open(csv_file_path, 'w', encoding='utf-8') as target_file:\n",
    "                            target_file.write(content)\n",
    "                    else:\n",
    "                        print(f\"File already in UTF-8: {csv_file_path}\")\n",
    "\n",
    "print(\"Download, extraction, file removal, and encoding conversion completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9bc4e",
   "metadata": {
    "id": "6ba9bc4e"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# create the datasets\n",
    "#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "bpa_df = None\n",
    "bpp_df = None\n",
    "dfc_mi_df = None\n",
    "dmpl_df = None\n",
    "dra_df = None\n",
    "dre_df = None\n",
    "dva_df = None\n",
    "\n",
    "# Load files into Data Frames\n",
    "for dir_name in os.listdir(download_dir):\n",
    "    if not dir_name.endswith(\".zip\"):\n",
    "        dir_path = os.path.join(download_dir, dir_name)\n",
    "\n",
    "        for csv_file_name in os.listdir(dir_path):\n",
    "            csv_file_path = os.path.join(dir_path, csv_file_name)\n",
    "            print(\"\\n-------------------------------------------------------\")\n",
    "            print(f\"Reading csv file: {csv_file_path}\")\n",
    "\n",
    "            csv_file_type = os.path.splitext(csv_file_name)[0][15:19].replace(\"_\", \"\")\n",
    "            print(f\"CSV type: {csv_file_type}\")\n",
    "\n",
    "            #local_df = pd.read_csv(csv_file_path, sep=';', index_col='CD_CVM')\n",
    "            local_df = pd.read_csv(csv_file_path, sep=';')\n",
    "            print(f\"Dataframe size: {len(local_df)}\")\n",
    "\n",
    "            match csv_file_type:\n",
    "                case 'BPA':\n",
    "                    if bpa_df is None:\n",
    "                        bpa_df = local_df\n",
    "                    else:\n",
    "                        bpa_df = pd.concat([bpa_df, local_df])\n",
    "                    print(f\"Dataframe size - after concat: {len(bpa_df)}\")\n",
    "\n",
    "                case 'BPP':\n",
    "                    if bpp_df is None:\n",
    "                        bpp_df = local_df\n",
    "                    else:\n",
    "                        bpp_df = pd.concat([bpp_df, local_df])\n",
    "                    print(f\"Dataframe size - after concat: {len(bpp_df)}\")\n",
    "\n",
    "                case 'DFC':\n",
    "                    if dfc_mi_df is None:\n",
    "                        dfc_mi_df = local_df\n",
    "                    else:\n",
    "                        dfc_mi_df = pd.concat([dfc_mi_df, local_df])\n",
    "                    print(f\"Dataframe size - after concat: {len(dfc_mi_df)}\")\n",
    "\n",
    "                case 'DMPL':\n",
    "                    if dmpl_df is None:\n",
    "                        dmpl_df = local_df\n",
    "                    else:\n",
    "                        dmpl_df = pd.concat([dmpl_df, local_df])\n",
    "                    print(f\"Dataframe size - after concat: {len(dmpl_df)}\")\n",
    "\n",
    "                case 'DRA':\n",
    "                    if dra_df is None:\n",
    "                        dra_df = local_df\n",
    "                    else:\n",
    "                        dra_df = pd.concat([dra_df, local_df])\n",
    "                    print(f\"Dataframe size - after concat: {len(dra_df)}\")\n",
    "\n",
    "                case 'DRE':\n",
    "                    if dre_df is None:\n",
    "                        dre_df = local_df\n",
    "                    else:\n",
    "                        dre_df = pd.concat([dre_df, local_df])\n",
    "                    print(f\"Dataframe size - after concat: {len(dre_df)}\")\n",
    "\n",
    "                case 'DVA':\n",
    "                    if dva_df is None:\n",
    "                        dva_df = local_df\n",
    "                    else:\n",
    "                        dva_df = pd.concat([dva_df, local_df])\n",
    "                    print(f\"Dataframe size - after concat: {len(dva_df)}\")\n",
    "\n",
    "# Adding missing column for BPA and BPP types\n",
    "bpa_df.insert(8, 'DT_INI_EXERC', bpa_df['DT_FIM_EXERC'].apply(lambda x: re.sub('(\\d{4}-\\d{2})-(\\d{2})', '\\\\1-01', x)))\n",
    "bpp_df.insert(8, 'DT_INI_EXERC', bpp_df['DT_FIM_EXERC'].apply(lambda x: re.sub('(\\d{4}-\\d{2})-(\\d{2})', '\\\\1-01', x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analising data\n",
    "\n",
    "def print_df(df_name, df):\n",
    "    print(f'\\n\\n{df_name}: {len(df):,} records')\n",
    "    print('-------------------------------------------------------------------')\n",
    "    print('\\t\\t\\t TYPES')\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print('\\t\\t\\t NULL VALUES')\n",
    "    print('CNPJ_CIA: ' + str(df['CNPJ_CIA'].isnull().sum().sum()))\n",
    "    print('DT_REFER: ' + str(df['DT_REFER'].isnull().sum().sum()))\n",
    "    print('VERSAO: ' + str(df['VERSAO'].isnull().sum().sum()))\n",
    "    print('DENOM_CIA: ' + str(df['DENOM_CIA'].isnull().sum().sum()))\n",
    "    print('CD_CVM: ' + str(df['CD_CVM'].isnull().sum().sum()))\n",
    "    print('GRUPO_DFP: ' + str(df['GRUPO_DFP'].isnull().sum().sum()))\n",
    "    print('MOEDA: ' + str(df['MOEDA'].isnull().sum().sum()))\n",
    "    print('ESCALA_MOEDA: ' + str(df['ESCALA_MOEDA'].isnull().sum().sum()))\n",
    "    print('ORDEM_EXERC: ' + str(df['ORDEM_EXERC'].isnull().sum().sum()))\n",
    "    print('DT_INI_EXERC: ' + str(df['DT_INI_EXERC'].isnull().sum().sum()))\n",
    "    print('DT_FIM_EXERC: ' + str(df['DT_FIM_EXERC'].isnull().sum().sum()))\n",
    "    print('CD_CONTA: ' + str(df['CD_CONTA'].isnull().sum().sum()))\n",
    "    print('DS_CONTA: ' + str(df['DS_CONTA'].isnull().sum().sum()))\n",
    "    print('VL_CONTA: ' + str(df['VL_CONTA'].isnull().sum().sum()))\n",
    "    print('ST_CONTA_FIXA: ' + str(df['ST_CONTA_FIXA'].isnull().sum().sum()))\n",
    "\n",
    "    print('\\t\\t\\t COUNT UNIQUE VALUES')\n",
    "    print('CNPJ_CIA: ' + str(len(df['CNPJ_CIA'].unique())))\n",
    "    print('DT_REFER: ' + str(len(df['DT_REFER'].unique())))\n",
    "    print('VERSAO: ' + str(len(df['VERSAO'].unique())))\n",
    "    print('DENOM_CIA: ' + str(len(df['DENOM_CIA'].unique())))\n",
    "    print('CD_CVM: ' + str(len(df['CD_CVM'].unique())))\n",
    "    print('GRUPO_DFP: ' + str(len(df['GRUPO_DFP'].unique())))\n",
    "    print('MOEDA: ' + str(len(df['MOEDA'].unique())))\n",
    "    print('ESCALA_MOEDA: ' + str(len(df['ESCALA_MOEDA'].unique())))\n",
    "    print('ORDEM_EXERC: ' + str(len(df['ORDEM_EXERC'].unique())))\n",
    "    print('DT_INI_EXERC: ' + str(len(df['DT_INI_EXERC'].unique())))\n",
    "    print('DT_FIM_EXERC: ' + str(len(df['DT_FIM_EXERC'].unique())))\n",
    "    print('CD_CONTA: ' + str(len(df['CD_CONTA'].unique())))\n",
    "    print('DS_CONTA: ' + str(len(df['DS_CONTA'].unique())))\n",
    "    print('VL_CONTA: ' + str(len(df['VL_CONTA'].unique())))\n",
    "    print('ST_CONTA_FIXA: ' + str(len(df['ST_CONTA_FIXA'].unique())))\n",
    "\n",
    "    print('\\t\\t\\t UNIQUE VALUES')\n",
    "    print('CNPJ_CIA: ' + str(len(df['CNPJ_CIA'].unique())))\n",
    "    print('DT_REFER: ' + str(len(df['DT_REFER'].unique())))\n",
    "    print('VERSAO: ' + str(df['VERSAO'].unique()))\n",
    "    print('DENOM_CIA: ' + str(len(df['DENOM_CIA'].unique())))\n",
    "    print('CD_CVM: ' + str(len(df['CD_CVM'].unique())))\n",
    "    print('GRUPO_DFP: ' + str(df['GRUPO_DFP'].unique()))\n",
    "    print('MOEDA: ' + str(df['MOEDA'].unique()))\n",
    "    print('ESCALA_MOEDA: ' + str(df['ESCALA_MOEDA'].unique()))\n",
    "    print('ORDEM_EXERC: ' + str(df['ORDEM_EXERC'].unique()))\n",
    "    print('DT_INI_EXERC: ' + str(len(df['DT_INI_EXERC'].unique())))\n",
    "    print('DT_FIM_EXERC: ' + str(len(df['DT_FIM_EXERC'].unique())))\n",
    "    print('CD_CONTA: ' + str(len(df['CD_CONTA'].unique())))\n",
    "    print('DS_CONTA: ' + str(len(df['DS_CONTA'].unique())))\n",
    "    print('VL_CONTA: ' + str(len(df['VL_CONTA'].unique())))\n",
    "    print('ST_CONTA_FIXA: ' + str(df['ST_CONTA_FIXA'].unique()))\n",
    "\n",
    "\n",
    "print_df('bpa_df', bpa_df)\n",
    "print_df('bpp_df', bpp_df)\n",
    "print_df('dfc_mi_df', dfc_mi_df)\n",
    "print_df('dmpl_df', dmpl_df)\n",
    "print_df('dra_df', dra_df)\n",
    "print_df('dre_df', dre_df)\n",
    "print_df('dva_df', dva_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3959829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30479985",
   "metadata": {
    "id": "30479985"
   },
   "source": [
    "#### Downloading stock market data from the CVM website.\n",
    "#### The data will consist of quarterly reports spanning from 2011 to 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a22943e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4540,
     "status": "ok",
     "timestamp": 1693529355133,
     "user": {
      "displayName": "Enok Antônio de Jesus",
      "userId": "15348291901420349452"
     },
     "user_tz": 180
    },
    "id": "7a22943e",
    "outputId": "0cc9306a-7982-4423-c4c5-75b9224324db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/enok/anaconda3/lib/python3.9/site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/enok/anaconda3/lib/python3.9/site-packages (4.11.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/enok/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/enok/anaconda3/lib/python3.9/site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/enok/anaconda3/lib/python3.9/site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/enok/anaconda3/lib/python3.9/site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/enok/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: chardet in /home/enok/anaconda3/lib/python3.9/site-packages (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# install libs\n",
    "#\n",
    "!pip install requests beautifulsoup4\n",
    "!pip install chardet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563bd89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135343,
     "status": "ok",
     "timestamp": 1693529493588,
     "user": {
      "displayName": "Enok Antônio de Jesus",
      "userId": "15348291901420349452"
     },
     "user_tz": 180
    },
    "id": "6563bd89",
    "outputId": "b2c3d71e-27dc-4d52-ace6-755b7f4f6077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: itr_cia_aberta_2011.zip\n",
      "Files already extracted: itr_cia_aberta_2011.zip -> downloaded_files/itr_cia_aberta_2011\n",
      "File already in UTF-8: downloaded_files/itr_cia_aberta_2011/itr_cia_aberta_2011.csv\n",
      "File already in UTF-8: downloaded_files/itr_cia_aberta_2011/itr_cia_aberta_BPA_con_2011.csv\n",
      "File already in UTF-8: downloaded_files/itr_cia_aberta_2011/itr_cia_aberta_BPA_ind_2011.csv\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# download balance sheets files\n",
    "#\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import zipfile\n",
    "import chardet\n",
    "\n",
    "# Function to check if a file's encoding is UTF-8\n",
    "def is_utf8(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            raw_data = file.read()\n",
    "            result = chardet.detect(raw_data)\n",
    "            return result['encoding'] == 'utf-8'\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# URL of the website containing the ZIP files\n",
    "base_url = \"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/ITR/DADOS/\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(base_url)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all links on the page\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "# Create a directory to save the downloaded files\n",
    "download_dir = \"downloaded_files\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the links and download ZIP files\n",
    "for link in links:\n",
    "    file_url = urljoin(base_url, link[\"href\"])\n",
    "    if file_url.endswith(\".zip\"):\n",
    "        zip_file_name = os.path.basename(file_url)\n",
    "        zip_subdir = os.path.splitext(zip_file_name)[0]  # Use ZIP file name without extension as subdirectory name\n",
    "        zip_subdir_path = os.path.join(download_dir, zip_subdir)\n",
    "\n",
    "        # Check if the ZIP file already exists\n",
    "        if not os.path.exists(zip_file_name):\n",
    "            print(f\"Downloading: {zip_file_name}\")\n",
    "            with open(zip_file_name, \"wb\") as file:\n",
    "                file_response = requests.get(file_url)\n",
    "                file.write(file_response.content)\n",
    "        else:\n",
    "            print(f\"File already exists: {zip_file_name}\")\n",
    "        \n",
    "        # Create a subdirectory for the ZIP file\n",
    "        os.makedirs(zip_subdir_path, exist_ok=True)\n",
    "\n",
    "        # Check if the ZIP file has already been extracted\n",
    "        if not os.listdir(zip_subdir_path):\n",
    "            print(f\"Unzipping: {zip_file_name} -> {zip_subdir_path}\")\n",
    "            with zipfile.ZipFile(zip_file_name, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(zip_subdir_path)\n",
    "        else:\n",
    "            print(f\"Files already extracted: {zip_file_name} -> {zip_subdir_path}\")\n",
    "\n",
    "        # Convert CSV files from ISO-8859-1 to UTF-8 only if they are not already UTF-8\n",
    "        for csv_file_name in os.listdir(zip_subdir_path):\n",
    "            if csv_file_name.endswith(\".csv\"):\n",
    "                csv_file_path = os.path.join(zip_subdir_path, csv_file_name)\n",
    "                if not is_utf8(csv_file_path):\n",
    "                    print(f\"Converting encoding: {csv_file_path}\")\n",
    "                    with open(csv_file_path, 'r', encoding='ISO-8859-1') as source_file:\n",
    "                        content = source_file.read()\n",
    "                    with open(csv_file_path, 'w', encoding='utf-8') as target_file:\n",
    "                        target_file.write(content)\n",
    "                else:\n",
    "                    print(f\"File already in UTF-8: {csv_file_path}\")\n",
    "\n",
    "print(\"Download, extraction, and encoding conversion completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba9bc4e",
   "metadata": {
    "id": "6ba9bc4e"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# create the datasets\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8b1c1",
   "metadata": {
    "id": "5dd8b1c1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
